{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装必要库\n",
    "% pip install opencv-python scikit-learn matplotlib numpy\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, ReLU, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 全局配置（适配小数据集）\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.data_dir = \"/content/drive/MyDrive/fingerprint-recognition/data\"\n",
    "        self.save_dir = \"/content/fingerprint_results\"\n",
    "        self.img_size = (128, 128)\n",
    "        self.feat_dim = 128  # 特征向量维度\n",
    "        self.sample_ratio = 1.0  # 小数据集：禁用抽样（改为100%使用）\n",
    "        self.epochs = 15\n",
    "        self.batch_size = 8  # 小数据集：减小批大小\n",
    "        self.learning_rate = 0.0001\n",
    "        self.margin = 0.2\n",
    "        self.match_threshold = 0.85\n",
    "        self.feat_db_path = os.path.join(self.save_dir, \"fingerprint_feat_db.npz\")\n",
    "\n",
    "# 初始化\n",
    "config = Config()\n",
    "os.makedirs(config.save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. 解析数据集：读取Google Drive中data文件夹的真实指纹数据\n",
    "def parse_and_group_fingerprints(data_dir):\n",
    "    finger_groups = {}\n",
    "    # 遍历data文件夹下的所有图片（支持子文件夹/直接放图片两种结构）\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            # 只处理指纹图片（支持bmp/png/jpg格式）\n",
    "            if file.lower().endswith((\".bmp\", \".png\", \".jpg\", \".jpeg\")):\n",
    "                # 方案1：按子文件夹划分指纹ID（推荐）\n",
    "                # 比如 data/指纹ID1/1.png → ID=指纹ID1\n",
    "                if root != data_dir:\n",
    "                    finger_id = os.path.basename(root)\n",
    "                # 方案2：按文件名划分（无ufer文件夹时）\n",
    "                # 比如 data/finger_001_1.png → ID=finger_001\n",
    "                else:\n",
    "                    fname = os.path.splitext(file)[0]\n",
    "                    finger_id = \"_\".join(fname.split(\"_\")[:-1]) if \"_\" in fname else fname\n",
    "                \n",
    "                # 保存真实图片路径\n",
    "                img_path = os.path.join(root, file)\n",
    "                if finger_id not in finger_groups:\n",
    "                    finger_groups[finger_id] = []\n",
    "                finger_groups[finger_id].append(img_path)\n",
    "    \n",
    "    # 过滤：只保留有≥1个样本的指纹ID\n",
    "    finger_groups = {fid: paths for fid, paths in finger_groups.items() if len(paths) >= 1}\n",
    "    \n",
    "    # 打印真实数据信息（关键！看是否读取到你的数据）\n",
    "    print(f\"解析完成！真实指纹ID数：{len(finger_groups)}\")\n",
    "    # 可选：打印每个ID的样本数，确认是否读取成功\n",
    "    for fid, paths in finger_groups.items():\n",
    "        print(f\"指纹ID {fid}：{len(paths)} 个样本\")\n",
    "    return finger_groups\n",
    "\n",
    "# 2. 图像预处理（不变）\n",
    "def preprocess_img(img_path, img_size):\n",
    "    try:\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            raise Exception(f\"无法读取图片：{img_path}\")\n",
    "        img = cv2.resize(img, img_size)\n",
    "        img = img / 255.0\n",
    "        img = np.expand_dims(img, axis=-1)\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"预处理失败：{e}\")\n",
    "        return None\n",
    "\n",
    "# 3. 三元组生成器（保留Gemini修复的__getitem__，但恢复读取真实图片）\n",
    "class TripletGenerator(tf.keras.utils.Sequence): # 注意：要加tf.keras.utils.，否则会报错\n",
    "    def __init__(self, finger_groups, img_size, batch_size):\n",
    "        self.finger_groups = finger_groups\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.finger_ids = list(finger_groups.keys())\n",
    "        self.valid_triplets = self._generate_valid_triplets()\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def _generate_valid_triplets(self):\n",
    "        triplets = []\n",
    "        # 处理无数据的情况（避免报错）\n",
    "        if len(self.finger_ids) == 0:\n",
    "            return [(\"dummy\", \"dummy\", \"dummy\")]\n",
    "        \n",
    "        # 遍历真实指纹ID生成三元组\n",
    "        for anchor_id in self.finger_ids:\n",
    "            anchor_paths = self.finger_groups[anchor_id]\n",
    "            if len(anchor_paths) == 0:\n",
    "                continue\n",
    "            # 锚点样本\n",
    "            anchor_path = random.choice(anchor_paths)\n",
    "            # 正样本（同一ID）\n",
    "            pos_path = random.choice(anchor_paths) if len(anchor_paths)>=1 else anchor_path\n",
    "            # 负样本（不同ID）\n",
    "            neg_ids = [id for id in self.finger_ids if id != anchor_id]\n",
    "            if len(neg_ids) == 0:\n",
    "                neg_path = anchor_path # 无其他ID时用自身（仅临时）\n",
    "            else:\n",
    "                neg_id = random.choice(neg_ids)\n",
    "                neg_path = random.choice(self.finger_groups[neg_id])\n",
    "            \n",
    "            triplets.append((anchor_path, pos_path, neg_path))\n",
    "        \n",
    "        # 确保至少有batch_size个三元组\n",
    "        if len(triplets) < self.batch_size:\n",
    "            triplets += triplets[:self.batch_size - len(triplets)]\n",
    "        return triplets\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(1, len(self.valid_triplets) // self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_triplets = self.valid_triplets[idx*self.batch_size : (idx+1)*self.batch_size]\n",
    "        if len(batch_triplets) < self.batch_size:\n",
    "            batch_triplets += self.valid_triplets[:self.batch_size - len(batch_triplets)]\n",
    "        \n",
    "        anchors, positives, negatives = [], [], []\n",
    "        for a_path, p_path, n_path in batch_triplets:\n",
    "            # 核心修改：读取真实图片（而非随机图）\n",
    "            if a_path != \"dummy\":\n",
    "                a_img = preprocess_img(a_path, self.img_size)\n",
    "                p_img = preprocess_img(p_path, self.img_size)\n",
    "                n_img = preprocess_img(n_path, self.img_size)\n",
    "            else:\n",
    "                # 无数据时用随机图兜底\n",
    "                a_img = np.random.rand(*self.img_size, 1).astype(np.float32)\n",
    "                p_img = np.random.rand(*self.img_size, 1).astype(np.float32)\n",
    "                n_img = np.random.rand(*self.img_size, 1).astype(np.float32)\n",
    "            \n",
    "            anchors.append(a_img)\n",
    "            positives.append(p_img)\n",
    "            negatives.append(n_img)\n",
    "        \n",
    "        # 转为张量\n",
    "        anchors = tf.convert_to_tensor(anchors, dtype=tf.float32)\n",
    "        positives = tf.convert_to_tensor(positives, dtype=tf.float32)\n",
    "        negatives = tf.convert_to_tensor(negatives, dtype=tf.float32)\n",
    "        return (anchors, positives, negatives), tf.zeros((len(anchors),), dtype=tf.float32)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        random.shuffle(self.valid_triplets)\n",
    "\n",
    "# 读取真实数据（你的Google Drive中的data文件夹）\n",
    "finger_groups = parse_and_group_fingerprints(config.data_dir)\n",
    "train_generator = TripletGenerator(finger_groups, config.img_size, config.batch_size)\n",
    "print(f\"三元组生成器初始化完成！每轮批次数：{len(train_generator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "# 基础CNN特征提取骨干网络（修正Input形状）\n",
    "def build_backbone(img_size, feat_dim):\n",
    "    # 核心修复：Input形状需包含通道维度（*img_size, 1）\n",
    "    inputs = Input(shape=(*img_size, 1))\n",
    "    x = Conv2D(16, (3,3), padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "    x = Conv2D(32, (3,3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "    x = Conv2D(64, (3,3), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(feat_dim)(x)\n",
    "    # 核心修复：将tf.nn.l2_normalize封装在Lambda层中\n",
    "    outputs = Lambda(lambda x: tf.nn.l2_normalize(x, axis=1), name='l2_normalize')(outputs)\n",
    "    return Model(inputs, outputs, name=\"fingerprint_backbone\")\n",
    "\n",
    "# 核心修复：用Keras层包装损失计算\n",
    "class TripletLossLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, margin=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.margin = margin\n",
    "\n",
    "    def call(self, inputs):\n",
    "        anchor_feat, pos_feat, neg_feat = inputs\n",
    "        # 计算距离（用Keras操作）\n",
    "        pos_dist = tf.reduce_sum(tf.square(anchor_feat - pos_feat), axis=1)\n",
    "        neg_dist = tf.reduce_sum(tf.square(anchor_feat - neg_feat), axis=1)\n",
    "        # 三元组损失\n",
    "        loss = tf.maximum(pos_dist - neg_dist + self.margin, 0.0)\n",
    "        self.add_loss(tf.reduce_mean(loss))  # 将损失添加到模型\n",
    "        return anchor_feat  # 仅返回锚点特征（无实际意义）\n",
    "\n",
    "# 构建三元组模型\n",
    "backbone = build_backbone(config.img_size, config.feat_dim)\n",
    "anchor_input = Input(shape=(*config.img_size, 1), name=\"anchor_input\")\n",
    "pos_input = Input(shape=(*config.img_size, 1), name=\"pos_input\")\n",
    "neg_input = Input(shape=(*config.img_size, 1), name=\"neg_input\")\n",
    "\n",
    "# 提取特征\n",
    "anchor_feat = backbone(anchor_input)\n",
    "pos_feat = backbone(pos_input)\n",
    "neg_feat = backbone(neg_input)\n",
    "\n",
    "# 核心修复：用自定义层计算损失\n",
    "outputs = TripletLossLayer(margin=config.margin)([anchor_feat, pos_feat, neg_feat])\n",
    "\n",
    "# 编译模型\n",
    "triplet_model = Model(inputs=[anchor_input, pos_input, neg_input], outputs=outputs)\n",
    "triplet_model.compile(optimizer=Adam(learning_rate=config.learning_rate))\n",
    "\n",
    "# 打印模型结构\n",
    "backbone.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "# 回调函数\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(config.save_dir, \"best_backbone.weights.h5\"), # 核心修复：文件名改为.weights.h5\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True\n",
    "    ),\n",
    "    EarlyStopping(monitor=\"loss\", patience=3, verbose=1)\n",
    "]\n",
    "\n",
    "# 开始训练\n",
    "print(\"开始训练特征提取器...\")\n",
    "history = triplet_model.fit(\n",
    "    train_generator,\n",
    "    epochs=config.epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 核心修复：\n",
    "# 1. 首先将保存的最佳权重加载回 tripet_model\n",
    "triplet_model.load_weights(os.path.join(config.save_dir, \"best_backbone.weights.h5\"))\n",
    "# 2. 从加载了最佳权重的 tripet_model 中，获取 backbone 层的权重\n",
    "best_backbone_weights = triplet_model.get_layer(\"fingerprint_backbone\").get_weights()\n",
    "# 3. 将这些权重设置到独立的 backbone 实例上\n",
    "backbone.set_weights(best_backbone_weights)\n",
    "\n",
    "# 保存最终的 backbone 模型\n",
    "backbone.save(os.path.join(config.save_dir, \"fingerprint_feat_extractor.h5\"))\n",
    "print(\"模型训练完成！\")\n",
    "\n",
    "# 绘制损失曲线\n",
    "plt.plot(history.history[\"loss\"], label=\"训练损失\")\n",
    "plt.title(\"三元组损失曲线\")\n",
    "plt.xlabel(\"轮数\")\n",
    "plt.ylabel(\"损失\")\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(config.save_dir, \"triplet_loss_curve.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "# 提取单张指纹特征\n",
    "def extract_fingerprint_feat(img_path, feat_extractor, img_size):\n",
    "    img = preprocess_img(img_path, img_size)\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    feat = feat_extractor.predict(img, verbose=0)[0]\n",
    "    return feat\n",
    "\n",
    "# 构建特征库\n",
    "def build_feat_database(finger_groups, feat_extractor, config):\n",
    "    feat_db = {}\n",
    "    print(\"构建特征库...\")\n",
    "    for fid, img_paths in finger_groups.items():\n",
    "        base_img_path = img_paths[0]\n",
    "        feat = extract_fingerprint_feat(base_img_path, feat_extractor, config.img_size)\n",
    "        if feat is not None:\n",
    "            feat_db[fid] = feat\n",
    "    # 保存\n",
    "    np.savez(config.feat_db_path, **feat_db)\n",
    "    print(f\"特征库保存完成！共{len(feat_db)}个指纹\")\n",
    "    return feat_db\n",
    "\n",
    "# 执行\n",
    "feat_db = build_feat_database(finger_groups, backbone, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载特征库\n",
    "def load_feat_database(feat_db_path):\n",
    "    if not os.path.exists(feat_db_path):\n",
    "        print(\"特征库不存在！\")\n",
    "        return {}\n",
    "    data = np.load(feat_db_path)\n",
    "    feat_db = {key: data[key] for key in data.files}\n",
    "    return feat_db\n",
    "\n",
    "# 比对函数\n",
    "def match_fingerprint(img_path, feat_extractor, feat_db, config):\n",
    "    # 提取输入特征\n",
    "    input_feat = extract_fingerprint_feat(img_path, feat_extractor, config.img_size)\n",
    "    if input_feat is None:\n",
    "        print(\"输入指纹特征提取失败！\")\n",
    "        return None, 0.0, False\n",
    "    \n",
    "    # 空特征库处理\n",
    "    if len(feat_db) == 0:\n",
    "        print(\"特征库为空！\")\n",
    "        return None, 0.0, False\n",
    "    \n",
    "    # 计算相似度\n",
    "    match_results = []\n",
    "    for fid, db_feat in feat_db.items():\n",
    "        sim = cosine_similarity([input_feat], [db_feat])[0][0]\n",
    "        match_results.append((fid, sim))\n",
    "    \n",
    "    # 排序并判断\n",
    "    match_results.sort(key=lambda x: x[1], reverse=True)\n",
    "    best_fid, best_sim = match_results[0]\n",
    "    is_match = best_sim >= config.match_threshold\n",
    "    \n",
    "    # 输出结果\n",
    "    print(\"\\n===== 比对结果 =====\")\n",
    "    print(f\"输入指纹：{img_path}\")\n",
    "    print(f\"最高相似度：{best_sim:.4f}（阈值：{config.match_threshold}）\")\n",
    "    if is_match:\n",
    "        print(f\"匹配成功！指纹ID：{best_fid}\")\n",
    "    else:\n",
    "        print(\"匹配失败：未找到相似指纹\")\n",
    "    return best_fid, best_sim, is_match\n",
    "\n",
    "# ========== 测试比对 ==========\n",
    "# 替换为你的测试指纹路径\n",
    "test_img_path = \"/content/fingerprint_dataset/finger_001_2.png\"\n",
    "\n",
    "# 加载特征库\n",
    "feat_db = load_feat_database(config.feat_db_path)\n",
    "# 执行比对\n",
    "match_id, similarity, is_match = match_fingerprint(test_img_path, backbone, feat_db, config)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
